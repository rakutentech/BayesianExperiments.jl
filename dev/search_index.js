var documenterSearchIndex = {"docs":
[{"location":"api/#Probabilistic-Models","page":"API","title":"Probabilistic Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ProbabilisticModel\nConjugateModel\nBernoulliModel\nExponentialModel\nNormalModel\nLogNormalModel\nChainedModel","category":"page"},{"location":"api/#BayesianExperiments.ProbabilisticModel","page":"API","title":"BayesianExperiments.ProbabilisticModel","text":"ProbabilisticModel\n\nProbabilisticModel is a a model of the parameters that we are interested in. The model is defined by its prior distribution and likelihood function.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ConjugateModel","page":"API","title":"BayesianExperiments.ConjugateModel","text":"ConjugateModel <: ProbabilisticModel\n\nConjugateModel is a ProbabilisticModel with a conjugate prior of the  corresponding likelihood function.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.BernoulliModel","page":"API","title":"BayesianExperiments.BernoulliModel","text":"BernoulliModel <: ConjugateModel\n\nBernoulli likelihood with Beta distribution as the conjugate prior.\n\nBernoulliModel(α, β)              # construct a BernoulliModel\n\nupdate!(model, stats)             # update model with statistics from data\nsample_post(model, numsamples)    # sampling from the posterior distribution\nsample_stats(model, numsamples)   # sampling statistics from the data generating distribution\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ExponentialModel","page":"API","title":"BayesianExperiments.ExponentialModel","text":"ExponentialModel <: ConjugateModel\n\nExponential likelihood with Gamma distribution as the conjugate prior.\n\nExponentialModel(α, β)            # construct a ExponentialModel\n\nupdate!(model, stats)             # update model with statistics from data\nsample_post(model, numsamples)    # sampling from the posterior distribution\nsample_stats(model, numsamples)   # sampling statistics from the data generating distribution\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.NormalModel","page":"API","title":"BayesianExperiments.NormalModel","text":"NormalModel <: ConjugateModel\n\nNormal likelihood and Normal Inverse Gamma distribution as the  conjugate prior.\n\nParameters\n\nμ: mean of normal distribution\nv:  scale variance of Normal \nα:  shape of Gamma distribution\nθ:  scale of Gamma distribution\n\nNormalModel(μ, v, α, θ)           # construct a NormalModel\n\nupdate!(model, stats)             # update model with statistics from data\nsample_post(model, numsamples)    # sampling from the posterior distribution\nsample_stats(model, numsamples)   # sampling statistics from the data generating distribution\n\nReferences\n\nThe update rule for Normal distribution is based on this lecture notes.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.LogNormalModel","page":"API","title":"BayesianExperiments.LogNormalModel","text":"LogNormalModel(μ, v, α, θ)\n\nA model with Normal likelihood and Normal Inverse distribution with log transformed data. Notice LogNormal in Distributions.jl takes mean and standard deviation of log(x)  instead of x as the input parameters.\n\nLogNormalModel(μ, v, α, θ) # construct a LogNormalModel\n\ntolognormalparams(μ_logx, σ²_logx) # convert normal parameters to log-normal parameters\nupdate!(model, stats)              # update model with statistics from data\nsample_post(model, numsamples)     # sampling from the posterior distribution\nsample_stats(model, numsamples)    # sampling statistics from the data generating distributio\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ChainedModel","page":"API","title":"BayesianExperiments.ChainedModel","text":"ChainedModel <: ProbabilisticModel\n\nChainedModel is a combination of ConjugateModels chained by the specified operator. It can be used to model a multiple step process.\n\n\n\n\n\n","category":"type"},{"location":"api/#Stopping-Rules","page":"API","title":"Stopping Rules","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ExpectedLossThresh\nProbabilityBeatAllThresh","category":"page"},{"location":"api/#BayesianExperiments.ExpectedLossThresh","page":"API","title":"BayesianExperiments.ExpectedLossThresh","text":"ExpectedLossThresh <: StoppingRule\n\nThe experiment has a winning model if the model has the smallest posterior expected loss,  and its expected loss value is below the threshold.\n\nReferences\n\nDefinition of the expected loss on Wikipedia\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ProbabilityBeatAllThresh","page":"API","title":"BayesianExperiments.ProbabilityBeatAllThresh","text":"ProbabilityBeatAllThresh <: StoppingRule\n\nThe experiment has a winning model if probability of that model's posterior samples  is larger than the alternative models is above the threshold.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experiment","page":"API","title":"Experiment","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Experiment\nExperimentABN\nExperimentAB","category":"page"},{"location":"api/#BayesianExperiments.Experiment","page":"API","title":"BayesianExperiments.Experiment","text":"Experiment\n\nAn experiment has the models to compare, a stopping rule to make decision. \n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ExperimentABN","page":"API","title":"BayesianExperiments.ExperimentABN","text":"ExperimentABN{T,n} <: Experiment\n\nAn experiment with stopping rule of type T and n models. Models must have the same ProbabilisticModel type.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ExperimentAB","page":"API","title":"BayesianExperiments.ExperimentAB","text":"ExperimentAB(models, rule; modelnames=nothing) where T <: StoppingRule\n\nAn experiment of ExperimentABN with two models.\n\n\n\n\n\n","category":"function"},{"location":"api/#Simulation","page":"API","title":"Simulation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Simulation","category":"page"},{"location":"api/#BayesianExperiments.Simulation","page":"API","title":"BayesianExperiments.Simulation","text":"Simulation(experiment, parameters, datagendists, maxsteps, onestepsizes, minsteps)\n\nA simulation setup includes the experiment, data generating distributions, max number of steps and minimum number of steps.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/sequential_experiment_two_models/#Sequential-Experiment-with-Two-Models","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"","category":"section"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"using Random \nusing Plots\n\nusing BayesianExperiments","category":"page"},{"location":"tutorials/sequential_experiment_two_models/#Two-Models","page":"Sequential Experiment with Two Models","title":"Two Models","text":"","category":"section"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"We want to compare two variants with ratios. For example, we have a new UI design and want to compare its CTR (click-through-rate) to the CTR of the existing design. In this case we can create two Bernoulli models to model the CTR's of each variant. ","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"A Bernoulli model is a model with Bernoulli distribution as the likelihood and Beta distribution as the prior. In the beginning, we set alpha=10 and beta=90 as parameters of the prior. (We will talk about the impact of priors and how to choose a \"proper\" prior for you experiment in details later).","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"α = 1\nβ = 1\nmodelA = BernoulliModel(α, β)\nmodelB = BernoulliModel(α, β)\nmodels = [modelA, modelB]\nmodelnames = [\"control\", \"variant 1\"];","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"We want to use \"expected loss\" as the key metric for our decision, and a \"threshold of caring\" of 0.0001 as the stopping rule of our experiment. An expected loss can be think as the \"average\" loss that we will get if our decision is wrong. It depends on how likely our decision is wrong, and how much loss we will get when our decision is wrong.","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"thresh = 1e-4\nstoppingrule = ExpectedLossThresh(thresh);","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"An experiment is a combination of underlying models and the stopping rule that we chose before.","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames);","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"Now we start to collect real data in our online experiment. Let's make some assumptions to demonstrate the idea:","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"The underlying data generating distributions follow Bernoulli distributions with parameters 0.01 and 0.0102. And That means our new design has 2% higher CTR than the current one.\nWe use 20 of the total traffic to test the new design. At each day, we can collect 4000 and 1000 observations for \"control\" and \"variant 1\". ","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"We will update the models as we collect more data and monitor the change of our expected loss. Our rule is quite simple: if the expected loss of one group is below the threshold, we can stop the experiment and make decision. And we will collect for maximum 15 days and declare no winner if there is no group with expected loss below the threshold.","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"max_days = 15\n\nRandom.seed!(12)\nexpected_losses = Vector{Float64}[]\nday = 0\nfor _ = 1:max_days\n    day += 1\n    # we collect data every day\n    dataA = rand(Bernoulli(0.0100), 4000)\n    dataB = rand(Bernoulli(0.0102), 1000)\n    \n    # convert data into statistics\n    statsA = BernoulliStatistics(dataA)\n    statsB = BernoulliStatistics(dataB)\n    \n    # update the models in the experiment with the statistics\n    update!(experiment, [statsA, statsB])\n    \n    # calculate the metrics (expected loss)\n    # this step is optional, for visualization below\n    _, losses = calculatemetrics(experiment)\n    push!(expected_losses, losses)\n    \n    # select winner, get \"nothing\" if there is no winner\n    winner = selectwinner!(experiment)\n    \n    # stop the experiment if we already find a winner\n    if winner !== nothing\n        break\n    end\nend","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"Now we can visualize the expected losses over days. We can see the expected loss of \"variant 1\" is decreasing over days. And at day 7 we find its expected loss is below the threshold in our stopping rule.","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"plot(collect(1:day), unnest(expected_losses), \n    title=\"Expected Losses\",\n    label=[\"Control\" \"Variant 1\"],\n    legend=:topleft,)\nhline!([thresh], color=:grey, line=:dash, label=:none)\nannotate!(1.5, 0.0002, text(\"Threshold\", 10))","category":"page"},{"location":"tutorials/sequential_experiment_two_models/","page":"Sequential Experiment with Two Models","title":"Sequential Experiment with Two Models","text":"(Image: svg)","category":"page"},{"location":"basic_examples/#Example:-Two-Models","page":"Getting Started","title":"Example: Two Models","text":"","category":"section"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"A basic example showing an experiment with two models.","category":"page"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA = rand(Bernoulli(0.15), n)\ndataB = rand(Bernoulli(0.16), n)\n\n# Define the models\nmodelA = BernoulliModel(1, 1)\nmodelB = BernoulliModel(1, 1)\n\n# Choose the stopping rule that we will use for making decision\nstoppingrule = ExpectedLossThresh(0.0002)\n\n# Setup the experiment by specifying models and the stopping rule\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Calculate the statistics from our sample data\nstatsA = BernoulliStatistics(dataA)\nstatsB = BernoulliStatistics(dataB)\n\n# Update the models in the experiment with the newly created statistics\nupdate!(experiment, [statsA, statsB])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = calculatemetrics(experiment)\n\n# Or, we can directly find the winning model in the experiment \nwinner = selectwinner!(experiment)","category":"page"},{"location":"basic_examples/#Example:-Three-Models","page":"Getting Started","title":"Example: Three Models","text":"","category":"section"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"Similar to the two models cases, but now we have three models.","category":"page"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA = rand(Bernoulli(0.150), n)\ndataB = rand(Bernoulli(0.145), n)\ndataC = rand(Bernoulli(0.152), n)\n\n# Define the models\nmodelA = BernoulliModel(1, 1)\nmodelB = BernoulliModel(1, 1)\nmodelC = BernoulliModel(1, 1)\n\n# Choose the stopping rule\nstoppingrule = ProbabilityBeatAllThresh(0.99)\n\n# Setup the experiment\nexperiment = ExperimentAB([modelA, modelB, modelC], stoppingrule)\n\n# Calculate the statistics from our sample data\nstatsA = BernoulliStatistics(dataA)\nstatsB = BernoulliStatistics(dataB)\nstatsC = BernoulliStatistics(dataC)\n\n# Update the model in the experiment with the newly created statistics\nupdate!(experiment, [statsA, statsB, statsC])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = calculatemetrics(experiment)\n\n# Or, we can directly find the winning model in the experiment \nwinner = selectwinner!(experiment)","category":"page"},{"location":"basic_examples/#Example:-Chained-Models","page":"Getting Started","title":"Example: Chained Models","text":"","category":"section"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"We have a chained model with BernoulliModel and LogNormalModel. A common use case is when we want to use revenue per visitor as the metric. We need to model distributions of both the conversion rate and revenue.  ","category":"page"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA1 = rand(Bernoulli(0.050), n)\ndataA2 = rand(LogNormal(1.0, 1.0), n)\ndataB1 = rand(Bernoulli(0.055), n)\ndataB2 = rand(LogNormal(1.0, 1.0), n)\n\n# Calculate the statistics from our sample data\nstatsA1 = BernoulliStatistics(dataA1)\nstatsA2 = LogNormalStatistics(dataA2)\nstatsB1 = BernoulliStatistics(dataB1)\nstatsB2 = LogNormalStatistics(dataB2)\n\n# Setup the experiment\nmodelA = ChainedModel(\n    [BernoulliModel(1, 1), LogNormalModel(0.0, 1.0, 0.001, 0.001)],\n    [ChainOperator.multiply]\n)\nmodelB = ChainedModel(\n    [BernoulliModel(1, 1), LogNormalModel(0.0, 1.0, 0.001, 0.001)],\n    [ChainOperator.multiply]\n)\n\n# Choose the stopping rule\nstoppingrule = ExpectedLossThresh(0.001)\n\n# Setup the experiment\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Update the model in the experiment with the newly created statistics\nupdate!(experiment, [[statsA1, statsA2], [statsB1, statsB2]])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = calculatemetrics(experiment)\n\n# Or, we can directly find the winning model in the experiment \nwinner = selectwinner!(experiment))","category":"page"},{"location":"basic_examples/#Example:-Power-Analysis","page":"Getting Started","title":"Example: Power Analysis","text":"","category":"section"},{"location":"basic_examples/","page":"Getting Started","title":"Getting Started","text":"# Choose the underlying data generating distributions\ndatagendists = [Bernoulli(0.2), Bernoulli(0.25)]\n\n# Choose the parameters of the simulation\n# Number of observations in each group in each step\nonestepsizes = [1000, 1000]\n\n# Maximum number of steps \nmaxsteps = 30\n\n# Minimum number of steps to run the simulation before \n# we apply the stopping rule\nminsteps = 5\n\n# Setup the experiment with models and stopping rule\nmodelA = BernoulliModel(1, 1)\nmodelB = BernoulliModel(1, 1)\nstoppingrule = ProbabilityBeatAllThresh(0.99)\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Setup the simulation\nsimulation = Simulation(\n    experiment=experiment,\n    datagendists=datagendists,\n    maxsteps=maxsteps,\n    onestepsizes=[10000, 10000],\n    minsteps=minsteps\n)\n\n# Run the simulation\nnumsteps, winners, _ = runsequential(\n    simulation, numsamples=1000, numsims=50)\n\n# Calculate the number of winning times for model B (\"variant 1\")\nsum(winners .== \"variant 1\") ==  2 ","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/#Type-S-Error-Analysis:-Comparing-Fixed-Horizon-and-Sequential-Experiment","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"","category":"section"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"A Type S error is an error of sign when our clamed sign of parameter is to the opposite of the truth. See detailed discussion in this blog article and paper.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"This example is taken from the post Is Bayesian A/B Testing Immune to Peeking? Not Exactly, which discusses about the impact of experiment design, fixed horizon or sequantial design, on the Type S error rate.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"using Random\nusing Plots\n\nusing BayesianExperiments","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"Suppose we have one new feature and we want to compare it to the old one. And the new feature will actually decrease our clickthrough rate from 01 to 09. We can use two Bernoulli distributions to represent the data generating processes.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"θ1 = 0.0010\nθ2 = 0.0009\ndatagendistA = Bernoulli(θ1)\ndatagendistB = Bernoulli(θ2)\ndatagendists = [datagendistA, datagendistB];","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"We start our model with priors α=10 and β=90.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"α = 10\nβ = 90\nmodelA = BernoulliModel(α, β)\nmodelB = BernoulliModel(α, β)\nmodels = [modelA, modelB]\nmodelnames = [\"old\", \"new\"];","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"We want to use \"expected loss\" as the key metric for our decision, and a \"threshold of caring\" of 0.00001 as the stopping rule of our experiment.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"thresh = 0.00001\nstoppingrule = ExpectedLossThresh(thresh);","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"Now we can setup the experiment. ","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames);","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/#Fixed-Horizon","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Fixed Horizon","text":"","category":"section"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"We can run simulation to analyze how will this experiment perform in reality.  For the first simulation, we want to use a fixed horizon that  we only see the result once after 20 days, and make decision at that point.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"simulation_fix = Simulation(\n    experiment=experiment,\n    datagendists=datagendists,\n    maxsteps=20,\n    onestepsizes=[10000, 10000],\n    minsteps=20\n)\n\nRandom.seed!(123)\nnumsteps, winners, metricvals = runsequential(simulation_fix, numsamples=10000, numsims=100)\nprintln(\"Ratio of new variant wins:\", sum(winners .== \"new\") / 100)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"Ratio of new variants wins:0.02","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/#Sequential-Experiment","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Sequential Experiment","text":"","category":"section"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"For the second simulation, we want to use sequential decision that  we will check the result at the end of each day","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"simulation_sequential = Simulation(\n    experiment=experiment,\n    datagendists=datagendists,\n    maxsteps=20,\n    onestepsizes=[10000, 10000],\n    minsteps=1\n)\n\nRandom.seed!(124)\nnumsteps, winners, metricvals = runsequential(simulation_sequential, numsamples=10000, numsims=100)\ntype_s_error_rate = sum(winners .== \"new\") / 100\nprintln(\"Ratio of new variant wins:\", type_s_error_rate)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"Ratio of new variants wins:0.11","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/#Expected-Loss-Thresholds-vs.-Resulting-Loss","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Expected Loss Thresholds vs. Resulting Loss","text":"","category":"section"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"thresholds = [collect(range(1e-6, 9e-6, length=5)); collect(range(1.1e-5, 1e-4, length=5))];","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"α = 100\nβ = 99900\nmodelA = BernoulliModel(α, β)\nmodelB = BernoulliModel(α, β)\nmodels = [modelA, modelB]\nmodelnames = [\"old\", \"new\"];","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"resulting_losses_fixed = Float64[]\ntype_s_error_rates_fixed = Float64[]\n\nnumsims = 200\nRandom.seed!(12)\nfor thresh in thresholds\n    stoppingrule = ExpectedLossThresh(thresh)\n    experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames)\n    simulation = Simulation(\n        experiment=experiment,\n        datagendists=datagendists,\n        maxsteps=20,\n        onestepsizes=[10000, 10000],\n        minsteps=20\n    )\n    numsteps, winners, metricvals = runsequential(simulation, numsamples=5000, numsims=numsims)\n    type_s_error_rate = sum(winners .== \"new\") / numsims\n    resulting_loss = (θ1 - θ2)*type_s_error_rate\n    push!(type_s_error_rates_fixed, type_s_error_rate)\n    push!(resulting_losses_fixed, resulting_loss)\n    @show thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss\nend","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.0e-6, 0.0, 0.0, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.0e-6, 5.000000000000002e-7, 0.005, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.0e-6, 5.000000000000002e-7, 0.005, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.0e-6, 2.0000000000000008e-6, 0.02, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (9.0e-6, 1.0000000000000004e-6, 0.01, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.1e-5, 2.0000000000000008e-6, 0.02, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.325e-5, 1.700000000000001e-5, 0.17, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.55e-5, 1.4000000000000008e-5, 0.14, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.775e-5, 1.4500000000000005e-5, 0.145, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (0.0001, 1.4000000000000008e-5, 0.14, true)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"resulting_losses_sequential = Float64[]\ntype_s_error_rates_sequential = Float64[]\n\nnumsims=200\nRandom.seed!(12)\nfor thresh in thresholds\n    stoppingrule = ExpectedLossThresh(thresh)\n    experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames)\n    simulation = Simulation(\n        experiment=experiment,\n        datagendists=datagendists,\n        maxsteps=20,\n        onestepsizes=[10000, 10000],\n        minsteps=1\n    )\n    numsteps, winners, metricvals = runsequential(simulation, numsamples=4000, numsims=numsims)\n    type_s_error_rate = sum(winners .== \"new\") / numsims\n    resulting_loss = (θ1 - θ2)*type_s_error_rate\n    push!(type_s_error_rates_sequential, type_s_error_rate)\n    push!(resulting_losses_sequential, resulting_loss)\n    @show thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss\nend","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.0e-6, 0.0, 0.0, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.0e-6, 1.0000000000000004e-6, 0.01, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.0e-6, 1.0000000000000004e-6, 0.01, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.0e-6, 3.500000000000002e-6, 0.035, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (9.0e-6, 5.000000000000003e-6, 0.05, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.1e-5, 6.500000000000003e-6, 0.065, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.325e-5, 2.8500000000000012e-5, 0.285, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.55e-5, 4.600000000000002e-5, 0.46, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.775e-5, 4.300000000000002e-5, 0.43, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (0.0001, 4.050000000000002e-5, 0.405, true)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"From the plot below we can find the resulting loss is always below the threshold set in our stopping rule.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"#Adding a small number to 0's to support log-scale plots.\nmask = resulting_losses_fixed .== 0.0\nresulting_losses_fixed[mask]  .= 1e-9\nmask = resulting_losses_sequential .== 0.0\nresulting_losses_sequential[mask]  .= 1e-9\n\nplot(thresholds, \n    [thresholds, resulting_losses_fixed, resulting_losses_sequential], \n    xaxis=:log, \n    yaxis=:log, \n    title=\"Resulting Loss vs. Expected Loss Threshold\",\n    label=[\"Expected Loss Thresh\" \"Resulting Loss (Fixed)\" \"Resulting Loss (Sequential)\"],\n    legend=:topleft, \n    xlabel=\"Threshold of expected loss\",\n    xguidefontsize=10\n)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"(Image: svg)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"The Type S error in the sequential experiment is more sensitive to the threshold we chose. As you can see from the plot below, the Type S error increases rapidly when the threshold gets bigger.","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"plot(thresholds, \n    [type_s_error_rates_fixed, type_s_error_rates_sequential], \n    title=\"Type S Error Rates\",\n    label=[\"Fixed\" \"Sequential\"],\n    legend=:topleft,\n    xticks=thresholds[5:end],\n    xlabel=\"Threshold of expected loss\",\n    xguidefontsize=10\n)","category":"page"},{"location":"tutorials/fixed_vs_sequentail_type_s_error/","page":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","title":"Type S Error Analysis: Comparing Fixed Horizon and Sequential Experiment","text":"(Image: svg)","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation of BayesianExperiments.jl, a library for conducting Bayesian AB testing in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Current features include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Conjugate prior models for distributions including Bernoulli, Normal, LogNormal, Exponential, etc.\nBasic models can be chained to model multiple steps process.\nVarious stopping rules support: expected loss, probability to beat all.\nSupport multiple experiment design including fixed horizon experiments, sequential experiment and online learning.\nEfficient Simulation tools to support power analysis.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install a stable version of BayesianExperiments by running the command in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ] add BayesianExperiments","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We welcome contributions to this project and discussion about its contents. Please open an issue or pull request on this repository to propose a change.","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple example showing how to use the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA = rand(Bernoulli(0.15), n)\ndataB = rand(Bernoulli(0.16), n)\n\n# Define the models\nmodelA = BernoulliModel(1, 1)\nmodelB = BernoulliModel(1, 1)\n\n# Choose the stopping rule that we will use for making decision\nstoppingrule = ExpectedLossThresh(0.0002)\n\n# Setup the experiment by specifying models and the stopping rule\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Calculate the statistics from our sample data\nstatsA = BernoulliStatistics(dataA)\nstatsB = BernoulliStatistics(dataB)\n\n# Update the models in the experiment with the newly created statistics\nupdate!(experiment, [statsA, statsB])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = calculatemetrics(experiment)","category":"page"}]
}
