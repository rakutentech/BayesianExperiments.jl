var documenterSearchIndex = {"docs":
[{"location":"api/#Conjugate-Models","page":"API","title":"Conjugate Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ProbabilisticModel\nConjugateModel\nConjugateBernoulli\nConjugateExponential\nConjugateNormal\nConjugateLogNormal\nChainedModel","category":"page"},{"location":"api/#BayesianExperiments.ProbabilisticModel","page":"API","title":"BayesianExperiments.ProbabilisticModel","text":"ProbabilisticModel\n\nProbabilisticModel is a a model of the parameters that we are interested in. The model is defined by its prior distribution and likelihood function.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ConjugateModel","page":"API","title":"BayesianExperiments.ConjugateModel","text":"ConjugateModel <: ProbabilisticModel\n\nConjugateModel is a ProbabilisticModel with a conjugate prior of the  corresponding likelihood function.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ConjugateBernoulli","page":"API","title":"BayesianExperiments.ConjugateBernoulli","text":"ConjugateBernoulli <: ConjugateModel\n\nBernoulli likelihood with Beta distribution as the conjugate prior.\n\nConjugateBernoulli(α, β)              # construct a ConjugateBernoulli\n\nupdate!(model, stats)             # update model with statistics from data\nsamplepost(model, numsamples)    # sampling from the posterior distribution\nsamplestats(model, numsamples)   # sampling statistics from the data generating distribution\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ConjugateExponential","page":"API","title":"BayesianExperiments.ConjugateExponential","text":"ConjugateExponential <: ConjugateModel\n\nExponential likelihood with Gamma distribution as the conjugate prior.\n\nConjugateExponential(α, β)            # construct a ConjugateExponential\n\nupdate!(model, stats)             # update model with statistics from data\nsamplepost(model, numsamples)    # sampling from the posterior distribution\nsamplestats(model, numsamples)   # sampling statistics from the data generating distribution\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ConjugateNormal","page":"API","title":"BayesianExperiments.ConjugateNormal","text":"ConjugateNormal <: ConjugateModel\n\nNormal likelihood and Normal Inverse Gamma distribution as the  conjugate prior.\n\nParameters\n\nμ: mean of normal distribution\nv:  scale variance of Normal \nα:  shape of Gamma distribution\nθ:  scale of Gamma distribution\n\nConjugateNormal(μ, v, α, θ)           # construct a ConjugateNormal\n\nupdate!(model, stats)             # update model with statistics from data\nsamplepost(model, numsamples)    # sampling from the posterior distribution\nsamplestats(model, numsamples)   # sampling statistics from the data generating distribution\n\nReferences\n\nThe update rule for Normal distribution is based on this  lecture notes.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ConjugateLogNormal","page":"API","title":"BayesianExperiments.ConjugateLogNormal","text":"ConjugateLogNormal(μ, v, α, θ)\n\nA model with Normal likelihood and Normal Inverse distribution with log transformed data. Notice LogNormal in Distributions.jl takes mean and standard deviation of log(x)  instead of x as the input parameters.\n\nConjugateLogNormal(μ, v, α, θ) # construct a ConjugateLogNormal\n\nlognormalparams(μ_logx, σ²_logx) # convert normal parameters to log-normal parameters\nupdate!(model, stats)              # update model with statistics from data\nsamplepost(model, numsamples)     # sampling from the posterior distribution\nsamplestats(model, numsamples)    # sampling statistics from the data generating distributio\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ChainedModel","page":"API","title":"BayesianExperiments.ChainedModel","text":"ChainedModel <: ProbabilisticModel\n\nChainedModel is a combination of ConjugateModels chained by the specified operator. It can be used to model a multiple step process.\n\n\n\n\n\n","category":"type"},{"location":"api/#Bayes-Factor-Models","page":"API","title":"Bayes Factor Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"NormalEffectSize\nStudentTEffectSize","category":"page"},{"location":"api/#BayesianExperiments.NormalEffectSize","page":"API","title":"BayesianExperiments.NormalEffectSize","text":"NormalEffectSize <: EffectSizeModel\n\nA standard effect size model has two hypotheses: H_0(null) an H_1(alternative):\n\nH_0: mu = m_0\nH_1: mu  m_0\n\nwith the population mean mu and pre-specified standard deviation sigma. We want to test whether  mu is equal to mu_0 or not.\n\nThe prior of the standard effect size is \n\ndelta  H_1 sim textNormal(0 sigma_0^2)\n\nwhere delta is the standard effect size. When sigma_0 = 1the prior is called  unit-information prior.  \n\nThe standard effect size delta is defined as \n\ndelta = fracmu - mu_0sigma\n\nIn practice, the standard deviations are unknown but in large sample scenario we assume  they are known and use their estimates.\n\nFileds\n\nμ0: mean of null hypothesis.\nσ0: The prior standard deviation of the effect size.\np0: prior belief of H_0. This is used to calculate the prior odds. Default 05.\n\nMethods\n\nbayesfactor(model, stats)    # calculate Bayes factor from one group statistics\nbayesfactor(model, twostats) # calculate Bayes factor from two group's statistics\n\nReferences\n\nChapter 5 hypothesis Testing with Normal Populations  in An Introduction to Bayesian Thinking.\nDeng, Alex, Jiannan Lu, and Shouyuan Chen. \"Continuous monitoring of A/B tests without pain:  Optional stopping in Bayesian testing.\" 2016 IEEE international conference on data science  and advanced analytics (DSAA). IEEE, 2016.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.StudentTEffectSize","page":"API","title":"BayesianExperiments.StudentTEffectSize","text":"StudentTEffectSize <: BayesFactorModel\n\nA model with Bayes factor from the Student's t distributions. We have a standard effect size model has two hypotheses: H_0(null) an H_1(alternative):\n\nH_0: mu = m_0\nH_1: mu  m_0\n\nThe model uses the Jeffreys-Zellener-Siow (JZS) prior. More specifically, we use a Cauchy prior on mu for H_1\n\nmu  sigma^2 sim textCauchy(0 r^2 sigma^2)\n\nand a Jeffrey's prior on sigma:\n\np(sigma^2) propto frac1sigma2\n\nfor both H_0 and H_1.\n\nFields\n\nr: Prior standard deviation of the effect size. \nrtol: Numerical tolerence fo the quadgk function used in the denominator calculation.\np0: prior belief of H_0. This is used to calculate the prior odds. Default 05.\n\nReferences\n\nRouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., & Iverson, G. (2009). Bayesian t tests  for accepting and rejecting the null hypothesis. Psychonomic bulletin & review, 16(2), 225-237.\n\n\n\n\n\n","category":"type"},{"location":"api/#Stopping-Rules","page":"API","title":"Stopping Rules","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ExpectedLossThresh\nProbabilityBeatAllThresh\nOneSidedBFThresh\nTwoSidedBFThresh","category":"page"},{"location":"api/#BayesianExperiments.ExpectedLossThresh","page":"API","title":"BayesianExperiments.ExpectedLossThresh","text":"ExpectedLossThresh <: StoppingRule\n\nThe experiment has a winning model if the model has the smallest posterior expected loss,  and its expected loss value is below the threshold.\n\nReferences\n\nDefinition of the expected loss on Wikipedia\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ProbabilityBeatAllThresh","page":"API","title":"BayesianExperiments.ProbabilityBeatAllThresh","text":"ProbabilityBeatAllThresh <: StoppingRule\n\nThe experiment has a winning model if probability of that model's posterior samples  is larger than the alternative models is above the threshold.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.OneSidedBFThresh","page":"API","title":"BayesianExperiments.OneSidedBFThresh","text":"OneSidedBFThresh <: BayesFactorThresh\n\nThe bayes factor itself is interpretable as the comparative evidence of data  under the two competing hypotheses. Higher bayes factor, as defined textBF_10 favours the alternative hypothesis. \n\nIn practice, a threshold can be used to make decision in bayes factor experiment. The experiment will stop when textBF_10 > threshold.\n\nIn this case, the bayes factor of alternative over null is above the threshold, we can accept the alternative hypothesis. Otherwise, we don't  have enough evidence to accept alternative hypotheses.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.TwoSidedBFThresh","page":"API","title":"BayesianExperiments.TwoSidedBFThresh","text":"TwoSidedBFThresh <: BayesFactorThresh\n\nThe bayes factor itself is interpretable as the comparative evidence of data  under the two competing hypotheses. Higher bayes factor, as defined textBF_10 favours the alternative hypothesis. \n\nIn practice, a threshold can be used to make decision in bayes factor experiment. The experiment will stop when \n\ntextBF_10 > threshold\ntextBF_10 < 1/threshold\n\nIn (1), the bayes factor of alternative over null is above the threshold, we can accept the alternative hypothesis. In (2), the bayes factor is below the inverse of the threshold, we can accept the null hypothesis. Otherwise, we don't  have enough evidence to accept any of these hypotheses.\n\n\n\n\n\n","category":"type"},{"location":"api/#Experiment","page":"API","title":"Experiment","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Experiment\nExperimentABN\nExperimentAB\nExperimentBF","category":"page"},{"location":"api/#BayesianExperiments.Experiment","page":"API","title":"BayesianExperiments.Experiment","text":"Experiment\n\nAn experiment has the models to compare, a stopping rule to make decision. \n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ExperimentABN","page":"API","title":"BayesianExperiments.ExperimentABN","text":"ExperimentABN{T,n} <: Experiment\n\nAn experiment with stopping rule of type T and n models. Models must have the same ProbabilisticModel type.\n\n\n\n\n\n","category":"type"},{"location":"api/#BayesianExperiments.ExperimentAB","page":"API","title":"BayesianExperiments.ExperimentAB","text":"ExperimentAB(models, rule; modelnames=nothing) where T <: StoppingRule\n\nAn experiment of ExperimentABN with two models.\n\n\n\n\n\n","category":"function"},{"location":"api/#BayesianExperiments.ExperimentBF","page":"API","title":"BayesianExperiments.ExperimentBF","text":"ExperimentBF{M} <: Experiment\n\nExperimentBF is an experiment using a Bayes Factor between the  null and alternative hypothesis as the stopping rule.\n\nConstructors\n\nExperimentBF(kwargs...)\n\nKeywords\n\nmodel::M: Prior of effect size of alternative hypothesis \np0::Float64: Probablity of null hypothesis\nwinner::Union{String, Nothing: Decision to reject the null hypothesis or not\nrule::BayesFactorThresh: Stopping rule using Bayes Factor as the threshold\nstats: Statistics for calculating the bayes factor. Default is nothing.\nmodelnames: Names of the hypotheses. Default is [\"null\", \"alternative\"].\n\n\n\n\n\n","category":"type"},{"location":"api/#Simulation","page":"API","title":"Simulation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Simulation","category":"page"},{"location":"api/#BayesianExperiments.Simulation","page":"API","title":"BayesianExperiments.Simulation","text":"Simulation(experiment, parameters, datagendists, maxsteps, onestepsizes, minsteps)\n\nA simulation setup includes the experiment, data generating distributions, max number of steps and minimum number of steps.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Getting Started","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"Pages = [\"examples.md\"]","category":"page"},{"location":"examples/#Examples:-Conjugate-Models","page":"Getting Started","title":"Examples: Conjugate Models","text":"","category":"section"},{"location":"examples/#Example:-Two-Models","page":"Getting Started","title":"Example: Two Models","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"A basic example showing an experiment with two models.","category":"page"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA = rand(Bernoulli(0.15), n)\ndataB = rand(Bernoulli(0.16), n)\n\n# Define the models\nmodelA = ConjugateBernoulli(1, 1)\nmodelB = ConjugateBernoulli(1, 1)\n\n# Choose the stopping rule that we will use for making decision\nstoppingrule = ExpectedLossThresh(0.0002)\n\n# Setup the experiment by specifying models and the stopping rule\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Calculate the statistics from our sample data\nstatsA = BernoulliStatistics(dataA)\nstatsB = BernoulliStatistics(dataB)\n\n# Update the models in the experiment with the newly created statistics\nupdate!(experiment, [statsA, statsB])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = metrics(experiment)\n\n# Or, we can directly find the winning model in the experiment \nwinner = decide!(experiment)","category":"page"},{"location":"examples/#Example:-Three-Models","page":"Getting Started","title":"Example: Three Models","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"Similar to the two models cases, but now we have three models.","category":"page"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA = rand(Bernoulli(0.150), n)\ndataB = rand(Bernoulli(0.145), n)\ndataC = rand(Bernoulli(0.180), n)\n\n# Define the models\nmodelA = ConjugateBernoulli(1, 1)\nmodelB = ConjugateBernoulli(1, 1)\nmodelC = ConjugateBernoulli(1, 1)\n\n# Choose the stopping rule\nstoppingrule = ProbabilityBeatAllThresh(0.99)\n\n# Setup the experiment\nexperiment = ExperimentABN([modelA, modelB, modelC], stoppingrule)\n\n# Calculate the statistics from our sample data\nstatsA = BernoulliStatistics(dataA)\nstatsB = BernoulliStatistics(dataB)\nstatsC = BernoulliStatistics(dataC)\n\n# Update the model in the experiment with the newly created statistics\nupdate!(experiment, [statsA, statsB, statsC])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = metrics(experiment)\n\n# Or, we can directly find the winning model in the experiment \nwinner = decide!(experiment)","category":"page"},{"location":"examples/#Example:-Chained-Models","page":"Getting Started","title":"Example: Chained Models","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"We have a chained model with ConjugateBernoulli and ConjugateLogNormal. A common use case is when we want to use revenue per visitor as the metric. We need to model distributions of both the conversion rate and revenue.  ","category":"page"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA1 = rand(Bernoulli(0.050), n)\ndataA2 = rand(LogNormal(1.0, 1.0), n)\ndataB1 = rand(Bernoulli(0.060), n)\ndataB2 = rand(LogNormal(1.0, 1.0), n)\n\n# Calculate the statistics from our sample data\nstatsA1 = BernoulliStatistics(dataA1)\nstatsA2 = LogNormalStatistics(dataA2)\nstatsB1 = BernoulliStatistics(dataB1)\nstatsB2 = LogNormalStatistics(dataB2)\n\n# Setup the experiment\nmodelA = ChainedModel(\n    [ConjugateBernoulli(1, 1), ConjugateLogNormal(0.0, 1.0, 0.001, 0.001)],\n    [MultiplyOperator()]\n)\nmodelB = ChainedModel(\n    [ConjugateBernoulli(1, 1), ConjugateLogNormal(0.0, 1.0, 0.001, 0.001)],\n    [MultiplyOperator()]\n)\n\n# Choose the stopping rule\nstoppingrule = ExpectedLossThresh(0.001)\n\n# Setup the experiment\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Update the model in the experiment with the newly created statistics\nupdate!(experiment, [[statsA1, statsA2], [statsB1, statsB2]])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = metrics(experiment)\n\n# Or, we can directly find the winning model in the experiment \nwinner = decide!(experiment)","category":"page"},{"location":"examples/#Example:-Power-Analysis","page":"Getting Started","title":"Example: Power Analysis","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"# Choose the underlying data generating distributions\ndatagendists = [Bernoulli(0.2), Bernoulli(0.25)]\n\n# Choose the parameters of the simulation\n# Number of observations in each group in each step\nonestepsizes = [1000, 1000]\n\n# Maximum number of steps \nmaxsteps = 30\n\n# Minimum number of steps to run the simulation before \n# we apply the stopping rule\nminsteps = 5\n\n# Setup the experiment with models and stopping rule\nmodelA = ConjugateBernoulli(1, 1)\nmodelB = ConjugateBernoulli(1, 1)\nstoppingrule = ProbabilityBeatAllThresh(0.99)\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Setup the simulation\nsimulation = Simulation(\n    experiment=experiment,\n    datagendists=datagendists,\n    maxsteps=maxsteps,\n    onestepsizes=[10000, 10000],\n    minsteps=minsteps\n)\n\n# Run the simulation\nnumsteps, winners, _ = runsequential(\n    simulation, numsamples=1000, numsims=50)\n\n# Calculate the number of winning times for model B (\"variant 1\")\nsum(winners .== \"variant 1\") ==  2 ","category":"page"},{"location":"examples/#Examples:-Bayes-Factor-Models","page":"Getting Started","title":"Examples: Bayes Factor Models","text":"","category":"section"},{"location":"examples/#Example:-One-group","page":"Getting Started","title":"Example: One group","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"This example uses the sleep data in R:","category":"page"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"# we have one group of sample\nsleepdata = [-1.2, -2.4, -1.3, -1.3, 0.0, -1.0, -1.8, -0.8, -4.6, -1.4]\n\n# calculate statistics\nnormalstats = NormalStatistics(sleepdata)\n\n# setup the prior model for the effect size\nmodel = StudentTEffectSize()\n\n# calculate the t-statistics\ntstats = StudentTStatistics(normalstats)\n\n# we can calculate the posterior odds (equal to bayes factor with\n# equal prior odds)\nbf = bayesfactor(model, stats)\n\n# we can setup the experiment after spcifying our stopping rule\nstoppingrule = TwoSidedBFThresh(9)\nexperiment = ExperimentBF(model=model, rule=stoppingrule)\n\n# update the experiment with the new data\nupdate!(experiment, normalstats)\n\n# make decision on the experiment\ndecide!(experiment)","category":"page"},{"location":"examples/#Example:-Two-groups","page":"Getting Started","title":"Example: Two groups","text":"","category":"section"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"This example uses the Chicken Weights by Feed Type in R.","category":"page"},{"location":"examples/","page":"Getting Started","title":"Getting Started","text":"horsebean=[179, 160, 136, 227, 217, 168, 108, 124, 143, 140]\nlinseed=[309, 229, 181, 141, 260, 203, 148, 169, 213, 257, 244, 271]\n\n# Calculate normal statistics from data in each group\nstats1 = NormalStatistics(horsebean)\nstats2 = NormalStatistics(linseed)\n\n# Here we assume equal variance\ntstat = StudentTStatistics(TwoNormalStatistics(stats1, stats2))\n\n# setup the prior model for the effect size\n# we can change the standard devation of the prior of effect size \nmodel = StudentTEffectSize(r=sqrt(2)/2)\n\n# we can calculate the posterior odds (equal to bayes factor with\n# equal prior odds)\nbf = bayesfactor(model, tstat)\n\n# we can setup the experiment after specifying our stopping rule\nstoppingrule = TwoSidedBFThresh(9)\nexperiment = ExperimentBF(model=model, rule=stoppingrule)\n\n# update the experiment with the new data\nupdate!(experiment, normalstats)\n\n# make decision on the experiment\ndecide!(experiment)","category":"page"},{"location":"tutorials/type_s_error/#Type-S-Error-in-Fixed-Horizon-and-Sequential-Testing-Experiment","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"","category":"section"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"A Type S error is an error of sign when our claimed sign of parameter is to the opposite of the truth. See detailed discussion in this blog article and paper.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"This example is taken from the post Is Bayesian A/B Testing Immune to Peeking? Not Exactly, which discusses about the impact of experiment design, fixed horizon or sequential design, on the Type S error rate.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"# Setup notebook running environment.\n# Please Be Patient: it might take a long time to \n# precompile these packages the first time you run \n# this notebook in your local environment.\nimport Pkg\nPkg.activate(\".\")\nPkg.instantiate();\n\nusing Random\nusing Plots\n\nusing BayesianExperiments","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"Suppose we have one new feature and we want to compare it to the old one. And the new feature will actually decrease our clickthrough rate from 01 to 09. We can use two Bernoulli distributions to represent the data generating processes.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"θ1 = 0.0010\nθ2 = 0.0009\ndatagendistA = Bernoulli(θ1)\ndatagendistB = Bernoulli(θ2)\ndatagendists = [datagendistA, datagendistB];","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"We start our model with priors α=10 and β=90.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"α = 10\nβ = 90\nmodelA = ConjugateBernoulli(α, β)\nmodelB = ConjugateBernoulli(α, β)\nmodels = [modelA, modelB]\nmodelnames = [\"old\", \"new\"];","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"We want to use \"expected loss\" as the key metric for our decision, and a \"threshold of caring\" of 0.00001 as the stopping rule of our experiment.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"thresh = 0.00001\nstoppingrule = ExpectedLossThresh(thresh);","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"Now we can setup the experiment. ","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames);","category":"page"},{"location":"tutorials/type_s_error/#Fixed-Horizon","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Fixed Horizon","text":"","category":"section"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"We can run simulation to analyze how will this experiment perform in reality.  For the first simulation, we want to use a fixed horizon that  we only see the result once after 20 days, and make decision at that point.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"simulation_fix = Simulation(\n    experiment=experiment,\n    datagendists=datagendists,\n    maxsteps=20,\n    onestepsizes=[10000, 10000],\n    minsteps=20\n)\n\nRandom.seed!(123)\nnumsteps, winners, metricvals = runsequential(simulation_fix, numsamples=10000, numsims=100)\nprintln(\"Ratio of new variant wins:\", sum(winners .== \"new\") / 100)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"Ratio of new variant wins:0.02","category":"page"},{"location":"tutorials/type_s_error/#Sequential-Experiment","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Sequential Experiment","text":"","category":"section"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"For the second simulation, we want to use sequential decision that  we will check the result at the end of each day","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"simulation_sequential = Simulation(\n    experiment=experiment,\n    datagendists=datagendists,\n    maxsteps=20,\n    onestepsizes=[10000, 10000],\n    minsteps=1\n)\n\nRandom.seed!(124)\nnumsteps, winners, metricvals = runsequential(simulation_sequential, numsamples=10000, numsims=100)\ntype_s_error_rate = sum(winners .== \"new\") / 100\nprintln(\"Ratio of new variant wins:\", type_s_error_rate)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"Ratio of new variant wins:0.11","category":"page"},{"location":"tutorials/type_s_error/#Expected-Loss-Thresholds-vs.-Resulting-Loss","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Expected Loss Thresholds vs. Resulting Loss","text":"","category":"section"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"thresholds = [collect(range(1e-6, 9e-6, length=5)); collect(range(1.1e-5, 1e-4, length=5))];","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"α = 100\nβ = 99900\nmodelA = ConjugateBernoulli(α, β)\nmodelB = ConjugateBernoulli(α, β)\nmodels = [modelA, modelB]\nmodelnames = [\"old\", \"new\"];","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"resulting_losses_fixed = Float64[]\ntype_s_error_rates_fixed = Float64[]\n\nnumsims = 200\nRandom.seed!(12)\nfor thresh in thresholds\n    stoppingrule = ExpectedLossThresh(thresh)\n    experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames)\n    simulation = Simulation(\n        experiment=experiment,\n        datagendists=datagendists,\n        maxsteps=20,\n        onestepsizes=[10000, 10000],\n        minsteps=20\n    )\n    numsteps, winners, metricvals = runsequential(simulation, numsamples=5000, numsims=numsims)\n    type_s_error_rate = sum(winners .== \"new\") / numsims\n    resulting_loss = (θ1 - θ2)*type_s_error_rate\n    push!(type_s_error_rates_fixed, type_s_error_rate)\n    push!(resulting_losses_fixed, resulting_loss)\n    @show thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss\nend","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.0e-6, 0.0, 0.0, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.0e-6, 5.000000000000002e-7, 0.005, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.0e-6, 5.000000000000002e-7, 0.005, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.0e-6, 2.0000000000000008e-6, 0.02, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (9.0e-6, 1.0000000000000004e-6, 0.01, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.1e-5, 2.0000000000000008e-6, 0.02, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.325e-5, 1.700000000000001e-5, 0.17, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.55e-5, 1.4000000000000008e-5, 0.14, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.775e-5, 1.4500000000000005e-5, 0.145, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (0.0001, 1.4000000000000008e-5, 0.14, true)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"resulting_losses_sequential = Float64[]\ntype_s_error_rates_sequential = Float64[]\n\nnumsims=200\nRandom.seed!(12)\nfor thresh in thresholds\n    stoppingrule = ExpectedLossThresh(thresh)\n    experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames)\n    simulation = Simulation(\n        experiment=experiment,\n        datagendists=datagendists,\n        maxsteps=20,\n        onestepsizes=[10000, 10000],\n        minsteps=1\n    )\n    numsteps, winners, metricvals = runsequential(simulation, numsamples=4000, numsims=numsims)\n    type_s_error_rate = sum(winners .== \"new\") / numsims\n    resulting_loss = (θ1 - θ2)*type_s_error_rate\n    push!(type_s_error_rates_sequential, type_s_error_rate)\n    push!(resulting_losses_sequential, resulting_loss)\n    @show thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss\nend","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.0e-6, 0.0, 0.0, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.0e-6, 1.0000000000000004e-6, 0.01, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.0e-6, 1.0000000000000004e-6, 0.01, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.0e-6, 3.500000000000002e-6, 0.035, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (9.0e-6, 5.000000000000003e-6, 0.05, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (1.1e-5, 6.500000000000003e-6, 0.065, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (3.325e-5, 2.8500000000000012e-5, 0.285, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (5.55e-5, 4.600000000000002e-5, 0.46, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (7.775e-5, 4.300000000000002e-5, 0.43, true)\n(thresh, resulting_loss, type_s_error_rate, thresh > resulting_loss) = (0.0001, 4.050000000000002e-5, 0.405, true)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"From the plot below we can find the resulting loss is always below the threshold set in our stopping rule.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"#Adding a small number to 0's to support log-scale plots.\nmask = resulting_losses_fixed .== 0.0\nresulting_losses_fixed[mask]  .= 1e-9\nmask = resulting_losses_sequential .== 0.0\nresulting_losses_sequential[mask]  .= 1e-9\n\nplot(thresholds, \n    [thresholds, resulting_losses_fixed, resulting_losses_sequential], \n    xaxis=:log, \n    yaxis=:log, \n    title=\"Resulting Loss vs. Expected Loss Threshold\",\n    label=[\"Expected Loss Thresh\" \"Resulting Loss (Fixed)\" \"Resulting Loss (Sequential)\"],\n    legend=:topleft, \n    xlabel=\"Threshold of expected loss\",\n    xguidefontsize=10\n)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"(Image: svg)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"The Type S error in the sequential experiment is more sensitive to the threshold we chose. As you can see from the plot below, the Type S error increases rapidly when the threshold gets bigger.","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"plot(thresholds, \n    [type_s_error_rates_fixed, type_s_error_rates_sequential], \n    title=\"Type S Error Rates\",\n    label=[\"Fixed\" \"Sequential\"],\n    legend=:topleft,\n    xticks=thresholds[5:end],\n    xlabel=\"Threshold of expected loss\",\n    xguidefontsize=10\n)","category":"page"},{"location":"tutorials/type_s_error/","page":"Type S Error in Fixed Horizon and Sequential Testing Experiment","title":"Type S Error in Fixed Horizon and Sequential Testing Experiment","text":"(Image: svg)","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/#Sequential-Testing-Experiment-with-Conjugate-Models","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"","category":"section"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"# Setup notebook running environment.\n# Please Be Patient: it might take a long time to \n# precompile these packages the first time you run \n# this notebook in your local environment.\nimport Pkg\nPkg.activate(\".\")\nPkg.instantiate();\n\nusing Random \nusing Plots\n\nusing BayesianExperiments","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/#Two-Models","page":"Sequential Testing Experiment with Conjugate Models","title":"Two Models","text":"","category":"section"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"We want to compare two variants with ratios. For example, we have a new UI design and want to compare its CTR (click-through-rate) to the CTR of the existing design. In this case we can create two Bernoulli models to model the CTR's of each variant. ","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"A Bernoulli model is a model with Bernoulli distribution as the likelihood and Beta distribution as the prior. In the beginning, we set alpha=10 and beta=90 as parameters of the prior. (We will talk about the impact of priors and how to choose a \"proper\" prior for you experiment in details later).","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"α = 1\nβ = 1\nmodelA = ConjugateBernoulli(α, β)\nmodelB = ConjugateBernoulli(α, β)\nmodels = [modelA, modelB]\nmodelnames = [\"control\", \"variant 1\"];","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"We want to use \"expected loss\" as the key metric for our decision, and a \"threshold of caring\" of 0.0001 as the stopping rule of our experiment. An expected loss can be think as the \"average\" loss that we will get if our decision is wrong. It depends on how likely our decision is wrong, and how much loss we will get when our decision is wrong.","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"thresh = 1e-4\nstoppingrule = ExpectedLossThresh(thresh);","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"An experiment is a combination of underlying models and the stopping rule that we chose before.","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"experiment = ExperimentAB(models, stoppingrule, modelnames=modelnames);","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"Now we start to collect real data in our online experiment. Let's make some assumptions to demonstrate the idea:","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"The underlying data generating distributions follow Bernoulli distributions with parameters 0.01 and 0.0102. And That means our new design has 2% higher CTR than the current one.\nWe use 20 of the total traffic to test the new design. At each day, we can collect 4000 and 1000 observations for \"control\" and \"variant 1\". ","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"We will update the models as we collect more data and monitor the change of our expected loss. Our rule is quite simple: if the expected loss of one group is below the threshold, we can stop the experiment and make decision. And we will collect for maximum 15 days and declare no winner if there is no group with expected loss below the threshold.","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"max_days = 15\n\nRandom.seed!(12)\nexpected_losses = Vector{Float64}[]\nday = 0\nfor _ = 1:max_days\n    day += 1\n    # we collect data every day\n    dataA = rand(Bernoulli(0.0100), 4000)\n    dataB = rand(Bernoulli(0.0102), 1000)\n    \n    # convert data into statistics\n    statsA = BernoulliStatistics(dataA)\n    statsB = BernoulliStatistics(dataB)\n    \n    # update the models in the experiment with the statistics\n    update!(experiment, [statsA, statsB])\n    \n    # calculate the metrics (expected loss)\n    # this step is optional, for visualization below\n    _, losses = metrics(experiment)\n    push!(expected_losses, losses)\n    \n    # select winner, get \"nothing\" if there is no winner\n    winner = decide!(experiment)\n    \n    # stop the experiment if we already find a winner\n    if winner !== nothing\n        break\n    end\nend","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"Now we can visualize the expected losses over days. We can see the expected loss of \"variant 1\" is decreasing over days. And at day 7 we find its expected loss is below the threshold in our stopping rule.","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"plot(collect(1:day), catbyrow(expected_losses), \n    title=\"Expected Losses\",\n    label=[\"Control\" \"Variant 1\"],\n    legend=:topleft,)\nhline!([thresh], color=:grey, line=:dash, label=:none)\nannotate!(1.5, 0.0002, text(\"Threshold\", 10))","category":"page"},{"location":"tutorials/sequential_testing_conjugate_models/","page":"Sequential Testing Experiment with Conjugate Models","title":"Sequential Testing Experiment with Conjugate Models","text":"(Image: svg)","category":"page"},{"location":"#BayesianExperiments.jl","page":"Home","title":"BayesianExperiments.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation of BayesianExperiments.jl, a library for conducting Bayesian AB testing in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Current features include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Hypothesis testing with Bayes factor. Support the effect size model with Normal distribution prior and JZS prior.\nBayesian decision making with conjugate prior models. Support expected loss and probability to beat all as the stopping rule.\nFlexible experiment design for both fixed horizon experiments and sequential test experiment.\nEfficient simulation tools to support power analysis and sensitivity analysis.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install a stable version of BayesianExperiments by running the command in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ] add BayesianExperiments","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple example showing how to use the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using BayesianExperiments\n\n# Generate sample data\nn = 1000\ndataA = rand(Bernoulli(0.15), n)\ndataB = rand(Bernoulli(0.16), n)\n\n# Define the models\nmodelA = ConjugateBernoulli(1, 1)\nmodelB = ConjugateBernoulli(1, 1)\n\n# Choose the stopping rule that we will use for making decision\nstoppingrule = ExpectedLossThresh(0.0002)\n\n# Setup the experiment by specifying models and the stopping rule\nexperiment = ExperimentAB([modelA, modelB], stoppingrule)\n\n# Calculate the statistics from our sample data\nstatsA = BernoulliStatistics(dataA)\nstatsB = BernoulliStatistics(dataB)\n\n# Update the models in the experiment with the newly created statistics\nupdate!(experiment, [statsA, statsB])\n\n# Calculate the metric (expected loss in this case) of each model \nwinner_index, expected_losses = metrics(experiment)","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We welcome contributions to this project and discussion about its contents. Please open an issue or pull request on this repository to propose a change.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Bayes-Factor-Experiment-with-Optional-Stopping","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"# Setup notebook running environment.\n# Please Be Patient: it might take a long time to \n# precompile these packages the first time you run \n# this notebook in your local environment.\nimport Pkg\nPkg.activate(\".\")\nPkg.instantiate();\n\nusing ProgressMeter: @showprogress\nusing DataFrames\n\nusing PrettyTables\nusing Random\nusing Query\nusing StatsPlots\nusing StatsPlots.PlotMeasures\nusing Plots\n\nusing BayesianExperiments\n\n# number of columns in a dataframe to show \nENV[\"COLUMNS\"] = 200;","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Optional stopping refers to the practice of peeking at data and make decision whether or not to continue an experiment. Such practice is usually prohibited in the frequentist AB testing framework. By using simulation-based result, Rouder (2014)[2] showed that a Bayes factor experiment with optional stopping can be valid with proper interpretation of the Bayesian quantities. ","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"This notebook follows the examples in Schönbrodt et al. (2016)[1] to conduct the error analysis of Bayes factor based experiment with optional stopping.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"The simulation will be conducted by following steps:","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Choose a threshold of Bayes factor for decision making. For example, if the threshold is set to 10, when a Bayes factor of textBF_10 is larger than 10, or less than 1/10, we decide we have collected enough evidence and stop the experiment.\nChoose a prior distribituion for the effect size under H_1. We will use the StudentTEffectSize model in the package. You can check the definition of NormalEffectSize model from the docstring by typing ?NormalEffectSize.\nRun a minimum number of steps (20 as the same in the paper), increase the sample size. Compute the bayes factor at each step.\nAs soon as the bayes factor value reached or exceeded the one of the thresholds as set in (1), or the maximum number of steps is reached, we will stop the experiment.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Some constants used in the simulation:","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Number of simulations: 5000\nMinimum number of steps: 20","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"The simulation function can be quickly created based on our package:","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"function simulate(δ, n, σ0; r=0.707, thresh=9, minsample=20)\n    # we will use two-sided decision rule for bayes factor\n    rule = TwoSidedBFThresh(thresh)\n    \n    # the prior distribution of effect size,\n    # r is the standard deviation\n    model = StudentTEffectSize(r=r)\n    \n    # setup the experiment\n    experiment = ExperimentBF(model=model, rule=rule)\n    \n    # create a sample with size n, the effect size is \n    # specified as δ\n    xs = rand(Normal(δ, 1), n)\n    \n    i = 0\n    # specify the stopping condition\n    while (i < n) & (experiment.winner === nothing)\n        i += 1\n        \n        # if minimum number of sample is not reached, \n        # keep collecting data\n        if i < minsample\n            continue\n        end\n        \n        stats = NormalStatistics(xs[1:i])\n        experiment.stats = stats\n        decide!(experiment)\n    end\n    experiment\nend\n\n# df table print helper\nprinttable(df) = pretty_table(df, tf=tf_markdown, nosubheader=true, header_crayon=Crayon(bold=:false))","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"printtable (generic function with 1 method)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Case-when-alternative-\\delta-0","page":"Bayes Factor Experiment with Optional Stopping","title":"Case when alternative delta = 0","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"When alternative delta  0, the error rate relates to the false positive rate. ","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"#deltas = collect(range(0, 1.5, step=0.2));\ndelta = 0.0\nrs = [0.707, 1.0, 1.414];\nthreshs = [3, 5, 7, 10];\ntotalnum = length(rs)*length(threshs);\n\nparamsgrid = reshape(collect(Base.Iterators.product(rs, threshs)), (totalnum, 1));\nparamsgrid = [(r=r, thresh=thresh) for (r, thresh) in paramsgrid];\n@show length(paramsgrid);","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"length(paramsgrid) = 12","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"n =  1000\nns = 5000\nminsample = 20\n\nsim_result1 = DataFrame(\n    delta=Float64[], \n    r=Float64[], \n    thresh=Float64[], \n    num_sim=Int64[], \n    num_null=Int64[], \n    num_alt=Int64[],\n    err_rate=Float64[], \n    avg_sample_size=Int64[])\n\n@showprogress for params in paramsgrid\n    delta = 0\n    r = params.r\n    thresh = params.thresh\n    winners = []\n    samplesizes = []\n    for _ in 1:ns\n        experiment = simulate(delta, n, r, thresh=thresh, minsample=minsample)\n        push!(winners, experiment.winner)\n        push!(samplesizes, experiment.stats.n)\n    end\n    \n    num_null = sum(winners .== \"null\")\n    num_alt = sum(winners .== \"alternative\")\n    \n    err_rate = num_alt/ns\n    avg_sample_size = mean(samplesizes)\n    push!(sim_result1, (delta, r, thresh, ns, num_null, num_alt, err_rate, convert(Int64, round(avg_sample_size))))\nend","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Progress: 100%|█████████████████████████████████████████| Time: 0:01:00","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"printtable(sim_result1)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"| delta |     r | thresh | num_sim | num_null | num_alt | err_rate | avg_sample_size |\n|-------|-------|--------|---------|----------|---------|----------|-----------------|\n|   0.0 | 0.707 |    3.0 |    5000 |     4680 |     320 |    0.064 |              24 |\n|   0.0 |   1.0 |    3.0 |    5000 |     4688 |     312 |   0.0624 |              24 |\n|   0.0 | 1.414 |    3.0 |    5000 |     4680 |     320 |    0.064 |              24 |\n|   0.0 | 0.707 |    5.0 |    5000 |     4731 |     268 |   0.0536 |              49 |\n|   0.0 |   1.0 |    5.0 |    5000 |     4734 |     266 |   0.0532 |              49 |\n|   0.0 | 1.414 |    5.0 |    5000 |     4725 |     275 |    0.055 |              49 |\n|   0.0 | 0.707 |    7.0 |    5000 |     4777 |     220 |    0.044 |             100 |\n|   0.0 |   1.0 |    7.0 |    5000 |     4742 |     247 |   0.0494 |              98 |\n|   0.0 | 1.414 |    7.0 |    5000 |     4753 |     238 |   0.0476 |             100 |\n|   0.0 | 0.707 |   10.0 |    5000 |     4744 |     191 |   0.0382 |             205 |\n|   0.0 |   1.0 |   10.0 |    5000 |     4762 |     184 |   0.0368 |             206 |\n|   0.0 | 1.414 |   10.0 |    5000 |     4751 |     183 |   0.0366 |             211 |","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Case-when-alternative-\\delta-0-2","page":"Bayes Factor Experiment with Optional Stopping","title":"Case when alternative delta  0","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"We create a grid of combinations of all parameters.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"deltas = collect(range(0.1, 1.0, step=0.2));\nrs = [0.707, 1.0, 1.414];\nthreshs = [3, 5, 7, 10];\ntotalnum = length(deltas)*length(rs)*length(threshs);\n\nparamsgrid = reshape(collect(Base.Iterators.product(deltas, rs, threshs)), (totalnum, 1));\nparamsgrid = [(delta=delta, r=r, thresh=thresh) for (delta, r, thresh) in paramsgrid]\n@show length(paramsgrid);\n@show paramsgrid[1:5];","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"length(paramsgrid) = 60\nparamsgrid[1:5] = NamedTuple{(:delta, :r, :thresh),Tuple{Float64,Float64,Int64}}[(delta = 0.1, r = 0.707, thresh = 3), (delta = 0.3, r = 0.707, thresh = 3), (delta = 0.5, r = 0.707, thresh = 3), (delta = 0.7, r = 0.707, thresh = 3), (delta = 0.9, r = 0.707, thresh = 3)]","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"The simulation is similar to the delta=0 case. When alternative delta  0, the error rate relates to the false negative evidence.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"n =  1000\nns = 5000\nminsample = 20\n\nsim_result2 = DataFrame(\n    delta=Float64[], \n    r=Float64[], \n    thresh=Float64[], \n    num_sim=Int64[], \n    num_null=Int64[], \n    num_alt=Int64[],\n    err_rate=Float64[], \n    avg_sample_size=Int64[])\n\n@showprogress for params in paramsgrid\n    delta=params.delta\n    r = params.r\n    thresh = params.thresh\n    winners = []\n    samplesizes = []\n    for _ in 1:ns\n        experiment = simulate(delta, n, r, thresh=thresh, minsample=minsample)\n        push!(winners, experiment.winner)\n        push!(samplesizes, experiment.stats.n)\n    end\n    \n    num_null = sum(winners .== \"null\")\n    num_alt = sum(winners .== \"alternative\")\n    err_rate = 1-num_alt/ns\n    avg_sample_size = mean(samplesizes)\n    push!(sim_result2, (delta, r, thresh, ns, num_null, num_alt, \n            err_rate, convert(Int64, round(avg_sample_size))))\nend","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Progress: 100%|█████████████████████████████████████████| Time: 0:02:46","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Simulation result when delta=05","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"sim_result2 |>\n    @filter(_.delta==0.5)|>\n    @orderby(_.delta) |> @thenby(_.r) |>\n    df -> printtable(DataFrame(df))","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"| delta |     r | thresh | num_sim | num_null | num_alt | err_rate | avg_sample_size |\n|-------|-------|--------|---------|----------|---------|----------|-----------------|\n|   0.5 | 0.707 |    3.0 |    5000 |      727 |    4273 |   0.1454 |              26 |\n|   0.5 | 0.707 |    5.0 |    5000 |       83 |    4917 |   0.0166 |              33 |\n|   0.5 | 0.707 |    7.0 |    5000 |        1 |    4999 |   0.0002 |              37 |\n|   0.5 | 0.707 |   10.0 |    5000 |        0 |    5000 |      0.0 |              39 |\n|   0.5 |   1.0 |    3.0 |    5000 |      759 |    4241 |   0.1518 |              26 |\n|   0.5 |   1.0 |    5.0 |    5000 |       69 |    4931 |   0.0138 |              34 |\n|   0.5 |   1.0 |    7.0 |    5000 |        2 |    4998 |   0.0004 |              37 |\n|   0.5 |   1.0 |   10.0 |    5000 |        0 |    5000 |      0.0 |              39 |\n|   0.5 | 1.414 |    3.0 |    5000 |      723 |    4277 |   0.1446 |              26 |\n|   0.5 | 1.414 |    5.0 |    5000 |       97 |    4903 |   0.0194 |              33 |\n|   0.5 | 1.414 |    7.0 |    5000 |        0 |    5000 |      0.0 |              36 |\n|   0.5 | 1.414 |   10.0 |    5000 |        0 |    5000 |      0.0 |              40 |","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Evaluate-the-simulation-result-with-Type-I-and-II-Error-and-FDR","page":"Bayes Factor Experiment with Optional Stopping","title":"Evaluate the simulation result with Type I & II Error and FDR","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"As pointed out by [2], we can evaluate the simulation result from the perspective of false discovery rate. Here we assume there is a 50-50 chance that the data is from either the null model or alternative model. ","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"We can merge the two simulations results by the prior standard deviation r and threshold of bayes factor. In the merged dataframe, each row represents a simulation with the 5000 samples from the null model and 5000 samples from the alternative model with the corresponding parameters (r, thresh, delta_1).","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"sim_result = leftjoin(sim_result1, sim_result2, \n    on=[:r, :thresh, :num_sim],\n    renamecols= \"_0\" => \"_1\"\n);\n\nsim_result.num_dis = sim_result.num_alt_0 + sim_result.num_alt_1;\nsim_result.num_false_dis = sim_result.num_alt_0;\nsim_result.fdr = sim_result.num_false_dis ./ sim_result.num_dis;\n\nsim_result.type1_error = sim_result.num_alt_0 ./ sim_result.num_sim;\nsim_result.type2_error = 1 .- sim_result.num_alt_1 ./ sim_result.num_sim;\nsim_result.power = sim_result.num_alt_1 ./ sim_result.num_sim;\nsim_result.avg_sample_size = (sim_result.avg_sample_size_0 + sim_result.avg_sample_size_1) ./ 2\n\nsim_result = sim_result |>\n    df -> select(df, [:delta_1, :r, :thresh, :num_sim, :num_alt_0, :num_alt_1, :type1_error,  \n                      :power, :fdr, :avg_sample_size]);","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Examples from merged dataframe:","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"sim_result |>\n    @filter(((_.delta_1 == 0.1) .& (_.r == 0.707)) .|\n            ((_.delta_1 == 0.1) .& (_.r == 1.0))  .|\n            ((_.delta_1 == 0.3) .& (_.r == 0.707)) .|\n            ((_.delta_1 == 0.3) .& (_.r == 1.0)) .|\n            ((_.delta_1 == 0.3) .& (_.r == 1.414))) |>\n    @orderby(_.delta_1) |> @thenby(_.r) |> @thenby(_.thresh) |>\n    df -> printtable(DataFrame(df))","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"| delta_1 |     r | thresh | num_sim | num_alt_0 | num_alt_1 | type1_error |  power |       fdr | avg_sample_size |\n|---------|-------|--------|---------|-----------|-----------|-------------|--------|-----------|-----------------|\n|     0.1 | 0.707 |    3.0 |    5000 |       320 |       518 |       0.064 | 0.1036 |  0.381862 |            24.5 |\n|     0.1 | 0.707 |    5.0 |    5000 |       268 |       846 |      0.0536 | 0.1692 |  0.240575 |            55.0 |\n|     0.1 | 0.707 |    7.0 |    5000 |       220 |      1387 |       0.044 | 0.2774 |  0.136901 |           126.0 |\n|     0.1 | 0.707 |   10.0 |    5000 |       191 |      1999 |      0.0382 | 0.3998 | 0.0872146 |           274.5 |\n|     0.1 |   1.0 |    3.0 |    5000 |       312 |       553 |      0.0624 | 0.1106 |  0.360694 |            24.5 |\n|     0.1 |   1.0 |    5.0 |    5000 |       266 |       839 |      0.0532 | 0.1678 |  0.240724 |            55.0 |\n|     0.1 |   1.0 |    7.0 |    5000 |       247 |      1301 |      0.0494 | 0.2602 |  0.159561 |           122.0 |\n|     0.1 |   1.0 |   10.0 |    5000 |       184 |      2064 |      0.0368 | 0.4128 | 0.0818505 |           274.5 |\n|     0.3 | 0.707 |    3.0 |    5000 |       320 |      2400 |       0.064 |   0.48 |  0.117647 |            26.0 |\n|     0.3 | 0.707 |    5.0 |    5000 |       268 |      3908 |      0.0536 | 0.7816 | 0.0641762 |            53.0 |\n|     0.3 | 0.707 |    7.0 |    5000 |       220 |      4767 |       0.044 | 0.9534 | 0.0441147 |            91.5 |\n|     0.3 | 0.707 |   10.0 |    5000 |       191 |      4990 |      0.0382 |  0.998 | 0.0368655 |           152.0 |\n|     0.3 |   1.0 |    3.0 |    5000 |       312 |      2386 |      0.0624 | 0.4772 |  0.115641 |            26.0 |\n|     0.3 |   1.0 |    5.0 |    5000 |       266 |      3951 |      0.0532 | 0.7902 |  0.063078 |            53.5 |\n|     0.3 |   1.0 |    7.0 |    5000 |       247 |      4752 |      0.0494 | 0.9504 | 0.0494099 |            90.5 |\n|     0.3 |   1.0 |   10.0 |    5000 |       184 |      4988 |      0.0368 | 0.9976 | 0.0355762 |           152.5 |\n|     0.3 | 1.414 |    3.0 |    5000 |       320 |      2414 |       0.064 | 0.4828 |  0.117045 |            26.0 |\n|     0.3 | 1.414 |    5.0 |    5000 |       275 |      3936 |       0.055 | 0.7872 | 0.0653052 |            53.0 |\n|     0.3 | 1.414 |    7.0 |    5000 |       238 |      4757 |      0.0476 | 0.9514 | 0.0476476 |            92.5 |\n|     0.3 | 1.414 |   10.0 |    5000 |       183 |      4993 |      0.0366 | 0.9986 | 0.0353555 |           155.0 |","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Visualizations","page":"Bayes Factor Experiment with Optional Stopping","title":"Visualizations","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/#Type-I-Error","page":"Bayes Factor Experiment with Optional Stopping","title":"Type I Error","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Some observations from the visualization below: ","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Higher thresholds will lower the Type I error rate.\nStandard deviation of prior of effect size (r). For lower value thresholds, lower r value will increase the Type I error. However, as the threshold increases, the r value seems to have smaller impact on the Type I error.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"# create labels for visualizations\nr_labels = hcat([\"r=$val\" for val in rs]...);\nthresh_labels = hcat([\"thresh=$(Int(val))\" for val in threshs]...);\ndelta_labels = hcat([\"\\\\delta=$(val)\" for val in deltas]...);","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"p0 = sim_result |>\n    @filter(_.delta_1==0.1) |>\n    @df plot(:thresh, [:type1_error], \n             group=(:r), \n             label=r_labels,\n             title=\"Bayes Factor Threshold vs Type I Error\",\n             xlabel=\"BF Threshold\",\n             ylabel=\"Type I Error\",\n             legend=true)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"(Image: svg)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Impact-of-Effect-Size-When-\\delta_1-0","page":"Bayes Factor Experiment with Optional Stopping","title":"Impact of Effect Size When delta_1  0","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"When the effect size gets larger, the power will increase and the FDR will decrease. ","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"p1 = sim_result |> \n    @filter(_.r==0.707) |> \n    @df plot(:delta_1, [:power], \n        group=(:thresh), \n        label=thresh_labels, \n        xlabel=\"Effect Size \\\\delta\",\n        ylabel=\"Power\");\n\np2 = sim_result |> \n    @filter(_.r==0.707) |> \n    @df plot(:delta_1, [:fdr], \n        group=(:thresh), \n        label=thresh_labels, \n        xlabel=\"Effect Size \\\\delta\",\n        ylabel=\"False Discovery Rate\");\n\nplot(p1, p2, size=(800, 400), layout=(1, 2), left_margin=[15mm 0mm]) ","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"(Image: svg)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Impact-of-Bayes-Factor-Thresholds","page":"Bayes Factor Experiment with Optional Stopping","title":"Impact of Bayes Factor Thresholds","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"When Bayes Factor threshold increases, the power also increase. This is because as the power increases, the chance we will falsely select the null hypothesis decreases.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"The plots below shows the relationship between Bayes factor thresholds and power for different effect sizes.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"function plot_thresh_vs_power(df, delta; xlabel=\"BF Threshold\")\n    return df |> @df plot(\n        plot(:thresh, [:power], group=(:r), label=r_labels, xlabel=xlabel, ylabel=\"Power\"),\n        plot(:thresh, [:fdr], group=(:r), label=r_labels, xlabel=xlabel, ylabel=\"FDR\"),\n        title =\"\\\\delta_{1} = $delta\")\nend\n\np1 = sim_result |>\n    @filter(_.delta_1==0.1) |>\n    df -> plot_thresh_vs_power(df, 0.1, xlabel=\"\");\n\np2 = sim_result |>\n    @filter(_.delta_1==0.3) |>\n    df -> plot_thresh_vs_power(df, 0.3, xlabel=\"\");\n\np3 = sim_result |>\n    @filter(_.delta_1==0.7) |>\n    df -> plot_thresh_vs_power(df, 0.7);\n\nplot(p1, p2, p3, layout=(3, 1), size=(800, 800))","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"(Image: svg)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#Average-Sample-Sizes-vs.-Bayes-Factor-Thresholds","page":"Bayes Factor Experiment with Optional Stopping","title":"Average Sample Sizes vs. Bayes Factor Thresholds","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"The average sample sizes needed to stop the experiment and make decision. As Bayes factor threshold gets larger, the expected sample sizes also get larger.","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"sim_result |> \n    @filter(_.r==0.707) |>\n    @df plot(:thresh, [:avg_sample_size], \n            group=(:delta_1), legend=:topleft, label=delta_labels,\n            xlabel=\"Bayes Factor Thresholds\", ylabel=\"Average Sample Size\")","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"(Image: svg)","category":"page"},{"location":"tutorials/bayes_factor_optional_stopping/#References","page":"Bayes Factor Experiment with Optional Stopping","title":"References","text":"","category":"section"},{"location":"tutorials/bayes_factor_optional_stopping/","page":"Bayes Factor Experiment with Optional Stopping","title":"Bayes Factor Experiment with Optional Stopping","text":"Schönbrodt, Felix D., Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. \"Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences.\" Psychological methods 22, no. 2 (2017): 322.\nDeng, Alex, Jiannan Lu, and Shouyuan Chen. \"Continuous monitoring of A/B tests without pain: Optional stopping in Bayesian testing.\" In 2016 IEEE international conference on data science and advanced analytics (DSAA), pp. 243-252. IEEE, 2016.\nRouder, Jeffrey N. \"Optional stopping: No problem for Bayesians.\" Psychonomic bulletin & review 21, no. 2 (2014): 301-308.","category":"page"}]
}
